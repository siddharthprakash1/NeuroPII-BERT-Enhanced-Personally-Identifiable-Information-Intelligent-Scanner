{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, mixed_precision\n",
    "from scipy.ndimage import zoom, rotate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AdamW(tf.keras.optimizers.Optimizer):\n",
    "    \"\"\"Custom AdamW optimizer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.004,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7,\n",
    "        amsgrad=False,\n",
    "        name=\"AdamW\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize optimizer.\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", learning_rate)\n",
    "        self._set_hyper(\"weight_decay\", weight_decay)\n",
    "        self._set_hyper(\"beta_1\", beta_1)\n",
    "        self._set_hyper(\"beta_2\", beta_2)\n",
    "        self.epsilon = epsilon\n",
    "        self.amsgrad = amsgrad\n",
    "        \n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"Create slots for optimizer variables.\"\"\"\n",
    "        # Create slots for first and second moments\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"m\")  # First moment\n",
    "            self.add_slot(var, \"v\")  # Second moment\n",
    "            if self.amsgrad:\n",
    "                self.add_slot(var, \"vhat\")  # Moved avg of second moment\n",
    "                \n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        \"\"\"Apply gradients to dense variables.\"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        weight_decay = self._get_hyper(\"weight_decay\", var_dtype)\n",
    "        beta_1_t = self._get_hyper(\"beta_1\", var_dtype)\n",
    "        beta_2_t = self._get_hyper(\"beta_2\", var_dtype)\n",
    "        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = tf.pow(beta_1_t, local_step)\n",
    "        beta_2_power = tf.pow(beta_2_t, local_step)\n",
    "\n",
    "        # Get slots\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        v = self.get_slot(var, \"v\")\n",
    "\n",
    "        # Adam updates\n",
    "        m_t = m.assign(\n",
    "            beta_1_t * m + (1.0 - beta_1_t) * grad,\n",
    "            use_locking=self._use_locking\n",
    "        )\n",
    "        v_t = v.assign(\n",
    "            beta_2_t * v + (1.0 - beta_2_t) * tf.square(grad),\n",
    "            use_locking=self._use_locking\n",
    "        )\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, \"vhat\")\n",
    "            vhat_t = vhat.assign(\n",
    "                tf.maximum(vhat, v_t),\n",
    "                use_locking=self._use_locking\n",
    "            )\n",
    "            denom = tf.sqrt(vhat_t) + epsilon_t\n",
    "        else:\n",
    "            denom = tf.sqrt(v_t) + epsilon_t\n",
    "\n",
    "        # Bias correction\n",
    "        lr_corr = lr_t * tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n",
    "        \n",
    "        # Update variable with momentum and weight decay\n",
    "        var_update = var.assign_sub(\n",
    "            lr_corr * m_t / denom + lr_t * weight_decay * var,\n",
    "            use_locking=self._use_locking\n",
    "        )\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "            \n",
    "        return tf.group(*updates)\n",
    "        \n",
    "    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "        \"\"\"Apply gradients to sparse variables.\"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        weight_decay = self._get_hyper(\"weight_decay\", var_dtype)\n",
    "        beta_1_t = self._get_hyper(\"beta_1\", var_dtype)\n",
    "        beta_2_t = self._get_hyper(\"beta_2\", var_dtype)\n",
    "        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = tf.pow(beta_1_t, local_step)\n",
    "        beta_2_power = tf.pow(beta_2_t, local_step)\n",
    "\n",
    "        # Get slots\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        v = self.get_slot(var, \"v\")\n",
    "\n",
    "        # Sparse updates for momentum\n",
    "        m_scaled_g_values = grad * (1 - beta_1_t)\n",
    "        m_t = m.assign(m * beta_1_t, use_locking=self._use_locking)\n",
    "        with tf.control_dependencies([m_t]):\n",
    "            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
    "\n",
    "        # Sparse updates for variance\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n",
    "        v_t = v.assign(v * beta_2_t, use_locking=self._use_locking)\n",
    "        with tf.control_dependencies([v_t]):\n",
    "            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, \"vhat\")\n",
    "            vhat_t = vhat.assign(\n",
    "                tf.maximum(vhat, v_t),\n",
    "                use_locking=self._use_locking\n",
    "            )\n",
    "            denom = tf.sqrt(vhat_t) + epsilon_t\n",
    "        else:\n",
    "            denom = tf.sqrt(v_t) + epsilon_t\n",
    "\n",
    "        # Bias correction\n",
    "        lr_corr = lr_t * tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n",
    "        \n",
    "        # Update variable with momentum\n",
    "        var_update = var.assign_sub(\n",
    "            lr_corr * m_t / denom + lr_t * weight_decay * var,\n",
    "            use_locking=self._use_locking\n",
    "        )\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "            \n",
    "        return tf.group(*updates)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Return configuration of the optimizer.\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"weight_decay\": self._serialize_hyperparameter(\"weight_decay\"),\n",
    "            \"beta_1\": self._serialize_hyperparameter(\"beta_1\"),\n",
    "            \"beta_2\": self._serialize_hyperparameter(\"beta_2\"),\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"amsgrad\": self.amsgrad,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "class ChannelAttention(layers.Layer):\n",
    "    def __init__(self, ratio=8, **kwargs):\n",
    "        super(ChannelAttention, self).__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channel = input_shape[-1]\n",
    "        self.shared_dense1 = layers.Dense(channel // self.ratio,\n",
    "                                        activation='relu',\n",
    "                                        kernel_initializer='he_normal',\n",
    "                                        use_bias=True,\n",
    "                                        bias_initializer='zeros')\n",
    "        self.shared_dense2 = layers.Dense(channel,\n",
    "                                        kernel_initializer='he_normal',\n",
    "                                        use_bias=True,\n",
    "                                        bias_initializer='zeros')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Average pooling\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2, 3], keepdims=True)\n",
    "        avg_pool = self.shared_dense1(avg_pool)\n",
    "        avg_pool = self.shared_dense2(avg_pool)\n",
    "\n",
    "        # Max pooling\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2, 3], keepdims=True)\n",
    "        max_pool = self.shared_dense1(max_pool)\n",
    "        max_pool = self.shared_dense2(max_pool)\n",
    "\n",
    "        attention = tf.nn.sigmoid(avg_pool + max_pool)\n",
    "        return attention\n",
    "\n",
    "class SpatialAttention(layers.Layer):\n",
    "    def __init__(self, kernel_size=7, **kwargs):\n",
    "        super(SpatialAttention, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv = layers.Conv3D(1, self.kernel_size, \n",
    "                                padding='same',\n",
    "                                kernel_initializer='he_normal',\n",
    "                                use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Average pooling along channel\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        # Max pooling along channel\n",
    "        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n",
    "        # Concatenate\n",
    "        concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
    "        # Apply convolution\n",
    "        spatial_attention = self.conv(concat)\n",
    "        return tf.nn.sigmoid(spatial_attention)\n",
    "\n",
    "class ChannelAttention(layers.Layer):\n",
    "    def __init__(self, ratio=8, **kwargs):\n",
    "        super(ChannelAttention, self).__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channel = input_shape[-1]\n",
    "        self.shared_dense1 = layers.Dense(channel // self.ratio,\n",
    "                                        activation='relu',\n",
    "                                        kernel_initializer='he_normal',\n",
    "                                        use_bias=True,\n",
    "                                        bias_initializer='zeros')\n",
    "        self.shared_dense2 = layers.Dense(channel,\n",
    "                                        kernel_initializer='he_normal',\n",
    "                                        use_bias=True,\n",
    "                                        bias_initializer='zeros')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Average pooling\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2, 3], keepdims=True)\n",
    "        avg_pool = self.shared_dense1(avg_pool)\n",
    "        avg_pool = self.shared_dense2(avg_pool)\n",
    "\n",
    "        # Max pooling\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2, 3], keepdims=True)\n",
    "        max_pool = self.shared_dense1(max_pool)\n",
    "        max_pool = self.shared_dense2(max_pool)\n",
    "\n",
    "        attention = tf.nn.sigmoid(avg_pool + max_pool)\n",
    "        return attention\n",
    "\n",
    "class SpatialAttention(layers.Layer):\n",
    "    def __init__(self, kernel_size=7, **kwargs):\n",
    "        super(SpatialAttention, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv = layers.Conv3D(1, self.kernel_size, \n",
    "                                padding='same',\n",
    "                                kernel_initializer='he_normal',\n",
    "                                use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Average pooling along channel\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        # Max pooling along channel\n",
    "        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n",
    "        # Concatenate\n",
    "        concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
    "        # Apply convolution\n",
    "        spatial_attention = self.conv(concat)\n",
    "        return tf.nn.sigmoid(spatial_attention)\n",
    "\n",
    "def create_attention_block(x, filters):\n",
    "    # Channel attention\n",
    "    channel_attention = ChannelAttention()(x)\n",
    "    x = layers.Multiply()([x, channel_attention])\n",
    "    \n",
    "    # Spatial attention\n",
    "    spatial_attention = SpatialAttention()(x)\n",
    "    x = layers.Multiply()([x, spatial_attention])\n",
    "    \n",
    "    return x\n",
    "    \n",
    "\n",
    "def create_residual_block(x, filters):\n",
    "    skip = layers.Conv3D(filters, kernel_size=1, padding='same')(x)\n",
    "    skip = layers.BatchNormalization()(skip)\n",
    "    \n",
    "    # First convolution block\n",
    "    x = layers.Conv3D(filters, kernel_size=3, padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "    # Second convolution block\n",
    "    x = layers.Conv3D(filters, kernel_size=3, padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Apply attention\n",
    "    x = create_attention_block(x, filters)\n",
    "    \n",
    "    # Residual connection\n",
    "    x = layers.Add()([x, skip])\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = layers.MaxPooling3D(pool_size=2)(x)\n",
    "    \n",
    "    return x \n",
    "\n",
    "class SingleScanDataGenerator:\n",
    "    def __init__(self, base_path, target_shape=(128, 128, 128)):\n",
    "        self.base_path = base_path\n",
    "        self.target_shape = target_shape\n",
    "        self.classes = ['AD', 'CN']  # Updated classes\n",
    "        \n",
    "    def preprocess_image(self, img):\n",
    "        \"\"\"Enhanced preprocessing pipeline\"\"\"\n",
    "        # Handle NaN and Inf values\n",
    "        img = np.nan_to_num(img)\n",
    "        \n",
    "        # Resize with better interpolation\n",
    "        zoom_factors = [self.target_shape[i] / img.shape[i] for i in range(3)]\n",
    "        img_resized = zoom(img, zoom_factors, order=2)  # Using order=2 for better interpolation\n",
    "        \n",
    "        # Robust normalization with outlier handling\n",
    "        p1, p99 = np.percentile(img_resized, (1, 99))\n",
    "        img_normalized = np.clip(img_resized, p1, p99)\n",
    "        img_normalized = (img_normalized - p1) / (p99 - p1)\n",
    "        \n",
    "        # Z-score normalization\n",
    "        mean = np.mean(img_normalized)\n",
    "        std = np.std(img_normalized) + 1e-10\n",
    "        img_normalized = (img_normalized - mean) / std\n",
    "        \n",
    "        return img_normalized\n",
    "    \n",
    "    def augment_image(self, img):\n",
    "        \"\"\"Enhanced data augmentation for training\"\"\"\n",
    "        # Random rotation in multiple planes\n",
    "        for axis in [(0,1), (0,2), (1,2)]:\n",
    "            if np.random.random() > 0.5:\n",
    "                angle = np.random.uniform(-20, 20)\n",
    "                img = rotate(img, angle, axes=axis, reshape=False)\n",
    "        \n",
    "        # Random flips on all axes\n",
    "        for axis in [0, 1, 2]:\n",
    "            if np.random.random() > 0.5:\n",
    "                img = np.flip(img, axis=axis)\n",
    "        \n",
    "        # Random intensity scaling\n",
    "        scale = np.random.uniform(0.85, 1.15)\n",
    "        img = img * scale\n",
    "        \n",
    "        # Random brightness adjustment\n",
    "        brightness = np.random.uniform(-0.1, 0.1)\n",
    "        img = img + brightness\n",
    "        \n",
    "        # Random gaussian noise\n",
    "        noise = np.random.normal(0, 0.02, img.shape)\n",
    "        img = img + noise\n",
    "        \n",
    "        # Random gamma correction\n",
    "        gamma = np.random.uniform(0.8, 1.2)\n",
    "        img = np.sign(img) * np.abs(img) ** gamma\n",
    "        \n",
    "        # Ensure values are in valid range\n",
    "        img = np.clip(img, -1, 1)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def get_data(self, subset='train'):\n",
    "        \"\"\"Get paths and labels with enhanced error handling\"\"\"\n",
    "        data_paths = []\n",
    "        labels = []\n",
    "        class_counts = {'AD': 0, 'CN': 0}\n",
    "        \n",
    "        subset_path = os.path.join(self.base_path,'ADNI', subset)  # Updated path\n",
    "        if not os.path.exists(subset_path):\n",
    "            raise ValueError(f\"Data path not found: {subset_path}\")\n",
    "            \n",
    "        for condition in self.classes:\n",
    "            condition_path = os.path.join(subset_path, condition)\n",
    "            if os.path.exists(condition_path):\n",
    "                scans = [f for f in os.listdir(condition_path) \n",
    "                        if f.endswith(('.nii', '.nii.gz'))]\n",
    "                \n",
    "                for scan in scans:\n",
    "                    scan_path = os.path.join(condition_path, scan)\n",
    "                    try:\n",
    "                        # Verify file can be loaded\n",
    "                        nib.load(scan_path)\n",
    "                        data_paths.append(scan_path)\n",
    "                        labels.append(condition)\n",
    "                        class_counts[condition] += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {scan_path}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        if not data_paths:\n",
    "            raise ValueError(\"No valid scans found in the data directory\")\n",
    "            \n",
    "        print(\"\\nClass distribution:\", class_counts)\n",
    "        print(f\"Total scans: {len(data_paths)}\")\n",
    "        return data_paths, labels\n",
    "    \n",
    "    def create_dataset(self, data_paths, labels, batch_size, augment=True, label_encoder=None):\n",
    "        \"\"\"Create optimized dataset for GPU training\"\"\"\n",
    "        if label_encoder is None:\n",
    "            label_encoder = LabelEncoder()\n",
    "            label_encoder.fit(labels)\n",
    "        \n",
    "        encoded_labels = label_encoder.transform(labels)\n",
    "        num_classes = len(label_encoder.classes_)\n",
    "        \n",
    "        def generator():\n",
    "            while True:\n",
    "                # Shuffle with numpy for better randomization\n",
    "                indices = np.random.permutation(len(data_paths))\n",
    "                for idx in indices:\n",
    "                    try:\n",
    "                        # Load and process data on CPU\n",
    "                        with tf.device('/CPU:0'):\n",
    "                            img = nib.load(data_paths[idx]).get_fdata()\n",
    "                            img = self.preprocess_image(img)\n",
    "                            \n",
    "                            if augment:\n",
    "                                img = self.augment_image(img)\n",
    "                            \n",
    "                            # Add channel dimension\n",
    "                            img = np.expand_dims(img, axis=-1)\n",
    "                            \n",
    "                            # Create one-hot encoded label\n",
    "                            label = tf.one_hot(encoded_labels[idx], num_classes)\n",
    "                            \n",
    "                            # Ensure types are correct\n",
    "                            img = tf.cast(img, tf.float32)\n",
    "                            label = tf.cast(label, tf.float32)\n",
    "                        \n",
    "                        yield img, label\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {data_paths[idx]}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        # Create dataset with proper types\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.target_shape + (1,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(num_classes,), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Optimize dataset for GPU training\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        dataset = dataset.cache()\n",
    "        \n",
    "        return dataset, label_encoder\n",
    "\n",
    "def create_3d_cnn(input_shape, num_classes):\n",
    "    \"\"\"Memory-optimized 3D CNN with residual connections and attention mechanisms\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial feature extraction with efficient bottleneck\n",
    "    x = layers.Conv3D(32, kernel_size=1, padding='same', kernel_regularizer=l2(1e-4))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "    # Progressive feature extraction with bottleneck blocks\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    for filters in filter_sizes:\n",
    "        # Bottleneck block\n",
    "        shortcut = x\n",
    "        \n",
    "        # Reduce channels\n",
    "        x = layers.Conv3D(filters // 4, kernel_size=1, padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "        \n",
    "        # Spatial convolution with reduced channels\n",
    "        x = layers.Conv3D(filters // 4, kernel_size=3, padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "        \n",
    "        # Expand channels\n",
    "        x = layers.Conv3D(filters, kernel_size=1, padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Project shortcut if needed\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv3D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        \n",
    "        # Add skip connection\n",
    "        x = layers.Add()([x, shortcut])\n",
    "        x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "        \n",
    "        # Reduce spatial dimensions\n",
    "        x = layers.MaxPooling3D(pool_size=2)(x)\n",
    "    \n",
    "    # Efficient global pooling\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Final classification\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    print(\"\\nGPU Information:\")\n",
    "    print(tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    data_gen = SingleScanDataGenerator(base_path='/kaggle/input/finaldata')\n",
    "    train_data_paths, train_labels = data_gen.get_data(subset='TRAIN')\n",
    "    test_data_paths, test_labels = data_gen.get_data(subset='TEST')\n",
    "  \n",
    "    # Initial setup code remains the same\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train_labels + test_labels)  # Fit on both train and test labels\n",
    "    class_counts = Counter(train_labels)  # Use train labels for class weights\n",
    "    total_samples = len(train_labels)  # Use train labels for total samples\n",
    "    \n",
    "    num_classes = len(class_counts)\n",
    "    class_weight_values = np.zeros(num_classes)\n",
    "    for i in range(num_classes):\n",
    "        count = class_counts[sorted(class_counts.keys())[i]]\n",
    "        class_weight_values[i] = (1 / count) * (total_samples / num_classes)\n",
    "    \n",
    "    class_weights = tf.constant(class_weight_values, dtype=tf.float32)\n",
    "    print(\"\\nClass weights:\", {i: w.numpy() for i, w in enumerate(class_weights)})\n",
    "    \n",
    "    # Data splitting and dataset creation remain the same\n",
    "    batch_size = 1\n",
    "    gradient_accumulation_steps = 8\n",
    "    \n",
    "    train_dataset, _ = data_gen.create_dataset(\n",
    "        train_data_paths, train_labels, batch_size, augment=True, label_encoder=label_encoder\n",
    "    )\n",
    "    test_dataset, _ = data_gen.create_dataset(\n",
    "        test_data_paths, test_labels, batch_size, augment=False, label_encoder=label_encoder\n",
    "    )\n",
    "    \n",
    "    steps_per_epoch = min(len(train_data_paths) // (batch_size * gradient_accumulation_steps), 50)\n",
    "    validation_steps = min(len(test_data_paths) // batch_size, 25)\n",
    "    \n",
    "    print(f\"\\nTraining with:\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"Validation steps: {validation_steps}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "    print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "    \n",
    "    # Create model and optimizer\n",
    "    model = create_3d_cnn((128, 128, 128, 1), num_classes)\n",
    "    \n",
    "    initial_learning_rate = 1e-4\n",
    "    decay_steps = 10000\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate,\n",
    "        decay_steps,\n",
    "        t_mul=2.0,\n",
    "        m_mul=0.9,\n",
    "        alpha=1e-5\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule,\n",
    "        weight_decay=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "    \n",
    "    # Create metrics\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "    train_auc = tf.keras.metrics.AUC(name='train_auc')\n",
    "    \n",
    "    val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "    val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n",
    "    val_auc = tf.keras.metrics.AUC(name='val_auc')\n",
    "    \n",
    "    # Define loss function\n",
    "    def weighted_categorical_focal_loss(y_true, y_pred, gamma=2.0):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        \n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        y_true_indices = tf.argmax(y_true, axis=-1)\n",
    "        sample_weights = tf.cast(tf.gather(class_weights, y_true_indices), tf.float32)\n",
    "        \n",
    "        ce_loss = -y_true * tf.math.log(y_pred)\n",
    "        pt = tf.exp(-ce_loss)\n",
    "        focal_loss = tf.pow(1 - pt, gamma) * ce_loss\n",
    "        weighted_focal_loss = focal_loss * tf.expand_dims(sample_weights, -1)\n",
    "        \n",
    "        return tf.reduce_mean(tf.reduce_sum(weighted_focal_loss, axis=-1))\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': [], 'auc': [], \n",
    "               'val_loss': [], 'val_accuracy': [], 'val_auc': []}\n",
    "    \n",
    "    # Initialize best metrics for model saving\n",
    "    best_val_accuracy = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Create directory for model checkpoints\n",
    "    os.makedirs('model_checkpoints', exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        print(f\"\\nEpoch {epoch + 1}\")\n",
    "        \n",
    "        # Reset metrics\n",
    "        train_loss.reset_state()\n",
    "        train_accuracy.reset_state()\n",
    "        train_auc.reset_state()\n",
    "        val_loss.reset_state()\n",
    "        val_accuracy.reset_state()\n",
    "        val_auc.reset_state()\n",
    "        \n",
    "        # Training\n",
    "        accumulated_gradients = None\n",
    "        \n",
    "        for step in range(steps_per_epoch * gradient_accumulation_steps):\n",
    "            x_batch, y_batch = next(iter(train_dataset))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(x_batch, training=True)\n",
    "                predictions = tf.cast(predictions, tf.float32)\n",
    "                loss = weighted_categorical_focal_loss(y_batch, predictions)\n",
    "                scaled_loss = loss / gradient_accumulation_steps\n",
    "            \n",
    "            gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "            gradients = [tf.clip_by_norm(g, 1.0) if g is not None else g for g in gradients]\n",
    "            \n",
    "            # Initialize or accumulate gradients\n",
    "            if accumulated_gradients is None:\n",
    "                accumulated_gradients = [tf.Variable(g) for g in gradients]\n",
    "            else:\n",
    "                for i, g in enumerate(gradients):\n",
    "                    accumulated_gradients[i].assign_add(g)\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss.update_state(loss)\n",
    "            train_accuracy.update_state(y_batch, predictions)\n",
    "            train_auc.update_state(y_batch, predictions)\n",
    "            \n",
    "            # Apply accumulated gradients\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
    "                accumulated_gradients = None\n",
    "        \n",
    "        # Validation\n",
    "        for x_val, y_val in test_dataset.take(validation_steps):\n",
    "            val_predictions = model(x_val, training=False)\n",
    "            val_predictions = tf.cast(val_predictions, tf.float32)\n",
    "            v_loss = weighted_categorical_focal_loss(y_val, val_predictions)\n",
    "            \n",
    "            val_loss.update_state(v_loss)\n",
    "            val_accuracy.update_state(y_val, val_predictions)\n",
    "            val_auc.update_state(y_val, val_predictions)\n",
    "        \n",
    "        # Update history\n",
    "        current_val_accuracy = val_accuracy.result().numpy()\n",
    "        current_val_loss = val_loss.result().numpy()\n",
    "        \n",
    "        history['loss'].append(train_loss.result().numpy())\n",
    "        history['accuracy'].append(train_accuracy.result().numpy())\n",
    "        history['auc'].append(train_auc.result().numpy())\n",
    "        history['val_loss'].append(current_val_loss)\n",
    "        history['val_accuracy'].append(current_val_accuracy)\n",
    "        history['val_auc'].append(val_auc.result().numpy())\n",
    "        \n",
    "        # Print current epoch metrics\n",
    "        print(f\"Loss: {train_loss.result():.4f}\")\n",
    "        print(f\"Accuracy: {train_accuracy.result():.4f}\")\n",
    "        print(f\"AUC: {train_auc.result():.4f}\")\n",
    "        print(f\"Val Loss: {current_val_loss:.4f}\")\n",
    "        print(f\"Val Accuracy: {current_val_accuracy:.4f}\")\n",
    "        print(f\"Val AUC: {val_auc.result():.4f}\")\n",
    "        \n",
    "        # Save model after every epoch\n",
    "        epoch_model_path = f'model_checkpoints/model_epoch_{epoch+1}.keras'\n",
    "        model.save(epoch_model_path)\n",
    "        print(f\"Saved epoch model: {epoch_model_path}\")\n",
    "        \n",
    "        # Save best model based on validation accuracy\n",
    "        if current_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = current_val_accuracy\n",
    "            model.save('model_checkpoints/best_model_accuracy.keras')\n",
    "            print(f\"\\nNew best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "            print(\"Saved best accuracy model\")\n",
    "            \n",
    "        # Save best model based on validation loss\n",
    "        if current_val_loss < best_val_loss:\n",
    "            best_val_loss = current_val_loss\n",
    "            model.save('model_checkpoints/best_model_loss.keras')\n",
    "            print(f\"\\nNew best validation loss: {best_val_loss:.4f}\")\n",
    "            print(\"Saved best loss model\")\n",
    "        \n",
    "        # Delete older epoch models to save space (keep only last 3 epochs)\n",
    "        existing_epoch_models = sorted([f for f in os.listdir('model_checkpoints') \n",
    "                                     if f.startswith('model_epoch_')])\n",
    "        if len(existing_epoch_models) > 3:\n",
    "            oldest_model = os.path.join('model_checkpoints', existing_epoch_models[0])\n",
    "            os.remove(oldest_model)\n",
    "    \n",
    "    # Save final model after training completion\n",
    "    final_model_path = 'model_checkpoints/final_model.keras'\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\nFinal model saved at: {final_model_path}\")\n",
    "    \n",
    "    # Create a summary file with model performances\n",
    "    with open('model_checkpoints/model_summary.txt', 'w') as f:\n",
    "        f.write(f\"Training Summary\\n\")\n",
    "        f.write(f\"================\\n\")\n",
    "        f.write(f\"Best validation accuracy: {best_val_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Best validation loss: {best_val_loss:.4f}\\n\")\n",
    "        f.write(f\"Final validation accuracy: {current_val_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Final validation loss: {current_val_loss:.4f}\\n\")\n",
    "    \n",
    "    return model, label_encoder, history, test_dataset\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Configure memory growth for GPU\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                try:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                except RuntimeError as e:\n",
    "                    print(e)\n",
    "        \n",
    "        # Create results directory\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        \n",
    "        # Train model\n",
    "        model, label_encoder, history, test_dataset = train_model()\n",
    "        \n",
    "        # Save the model\n",
    "        model.save('results/alzheimers_classification_model.keras')\n",
    "        print(\"\\nModel saved as 'results/alzheimers_classification_model.keras'\")\n",
    "                        \n",
    "        def create_visualization_results(model, history, test_dataset, label_encoder, save_dir='./', validation_steps=25):\n",
    "            \"\"\"\n",
    "            Create comprehensive visualizations for model analysis with progress tracking\n",
    "            \"\"\"\n",
    "            print(\"\\nGenerating visualizations...\")\n",
    "            \n",
    "            # Create results directory if it doesn't exist\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            print(\"1/5: Creating training history plots...\")\n",
    "            # Set style for better visualizations\n",
    "            plt.style.use('seaborn-v0_8-darkgrid')\n",
    "            \n",
    "            # 1. Training History Plot\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            \n",
    "            # Plot training metrics\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "            plt.plot(history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "            plt.title('Model Accuracy', fontsize=14, pad=10)\n",
    "            plt.xlabel('Epoch', fontsize=12)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.legend(fontsize=10)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.plot(history['loss'], label='Training Loss', linewidth=2)\n",
    "            plt.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "            plt.title('Model Loss', fontsize=14, pad=10)\n",
    "            plt.xlabel('Epoch', fontsize=12)\n",
    "            plt.ylabel('Loss', fontsize=12)\n",
    "            plt.legend(fontsize=10)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(history['auc'], label='Training AUC', linewidth=2)\n",
    "            plt.plot(history['val_auc'], label='Validation AUC', linewidth=2)\n",
    "            plt.title('Model AUC', fontsize=14, pad=10)\n",
    "            plt.xlabel('Epoch', fontsize=12)\n",
    "            plt.ylabel('AUC', fontsize=12)\n",
    "            plt.legend(fontsize=10)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Add learning curve analysis\n",
    "            plt.subplot(2, 2, 4)\n",
    "            train_sizes = np.linspace(0.1, 1.0, len(history['accuracy']))\n",
    "            plt.plot(train_sizes, history['accuracy'], 'o-', label='Training Accuracy', linewidth=2)\n",
    "            plt.plot(train_sizes, history['val_accuracy'], 'o-', label='Validation Accuracy', linewidth=2)\n",
    "            plt.title('Learning Curve Analysis', fontsize=14, pad=10)\n",
    "            plt.xlabel('Training Set Size (fraction)', fontsize=12)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.legend(fontsize=10)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{save_dir}/training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"2/5: Making predictions on test dataset...\")\n",
    "            # Get predictions on test set - limiting to validation_steps\n",
    "            test_batches = test_dataset.take(validation_steps)\n",
    "            y_pred_proba_list = []\n",
    "            y_true_list = []\n",
    "            \n",
    "            print(\"Processing test batches...\")\n",
    "            for i, (x_batch, y_batch) in enumerate(test_batches):\n",
    "                if i >= validation_steps:\n",
    "                    break\n",
    "                batch_pred = model.predict(x_batch, verbose=0)\n",
    "                y_pred_proba_list.append(batch_pred)\n",
    "                y_true_list.append(np.argmax(y_batch, axis=1))\n",
    "                if (i + 1) % 5 == 0:\n",
    "                    print(f\"Processed {i + 1}/{validation_steps} batches\")\n",
    "            \n",
    "            y_pred_proba = np.vstack(y_pred_proba_list)\n",
    "            y_true = np.concatenate(y_true_list)\n",
    "            y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "            \n",
    "            print(\"3/5: Generating ROC curve...\")\n",
    "            # Plot ROC Curve\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "            \n",
    "            for i in range(len(label_encoder.classes_)):\n",
    "                y_true_binary = (y_true == i).astype(int)\n",
    "                y_score = y_pred_proba[:, i]\n",
    "                fpr[i], tpr[i], _ = roc_curve(y_true_binary, y_score)\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "                \n",
    "                plt.plot(fpr[i], tpr[i], label=f'{label_encoder.classes_[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "            \n",
    "            plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.50)')\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)\n",
    "            plt.legend(fontsize=10)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(f'{save_dir}/roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"4/5: Creating confusion matrix...\")\n",
    "            # Enhanced Confusion Matrix\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            sns.heatmap(cm_normalized, annot=cm, fmt='d', cmap='Blues',\n",
    "                        xticklabels=label_encoder.classes_,\n",
    "                        yticklabels=label_encoder.classes_,\n",
    "                        annot_kws={'size': 12})\n",
    "            plt.title('Confusion Matrix\\n(Numbers: Raw Counts, Colors: Normalized)', fontsize=14, pad=20)\n",
    "            plt.xlabel('Predicted Label', fontsize=12)\n",
    "            plt.ylabel('True Label', fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{save_dir}/confusion_matrix_enhanced.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"5/5: Analyzing training progress...\")\n",
    "            # Training Progress Analysis\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            epochs = range(1, len(history['accuracy']) + 1)\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            acc_improvement = np.diff(history['val_accuracy'])\n",
    "            plt.plot(epochs[1:], acc_improvement, 'b-', label='Validation Accuracy Improvement')\n",
    "            plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "            plt.title('Validation Accuracy Improvement per Epoch', fontsize=12)\n",
    "            plt.xlabel('Epoch', fontsize=10)\n",
    "            plt.ylabel('Accuracy Improvement', fontsize=10)\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            loss_improvement = -np.diff(history['val_loss'])\n",
    "            plt.plot(epochs[1:], loss_improvement, 'g-', label='Validation Loss Improvement')\n",
    "            plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "            plt.title('Validation Loss Improvement per Epoch', fontsize=12)\n",
    "            plt.xlabel('Epoch', fontsize=10)\n",
    "            plt.ylabel('Loss Improvement', fontsize=10)\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{save_dir}/training_progress_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Create and save performance summary\n",
    "            performance_summary = {\n",
    "                'Final Training Accuracy': history['accuracy'][-1],\n",
    "                'Final Validation Accuracy': history['val_accuracy'][-1],\n",
    "                'Best Validation Accuracy': max(history['val_accuracy']),\n",
    "                'Final Training Loss': history['loss'][-1],\n",
    "                'Final Validation Loss': history['val_loss'][-1],\n",
    "                'Best Validation Loss': min(history['val_loss']),\n",
    "                'Final Training AUC': history['auc'][-1],\n",
    "                'Final Validation AUC': history['val_auc'][-1],\n",
    "                'Best Validation AUC': max(history['val_auc']),\n",
    "                'Number of Epochs': len(history['accuracy'])\n",
    "            }\n",
    "            \n",
    "            # Save performance summary\n",
    "            with open(f'{save_dir}/performance_summary.txt', 'w') as f:\n",
    "                f.write('Model Performance Summary\\n')\n",
    "                f.write('=======================\\n\\n')\n",
    "                for metric, value in performance_summary.items():\n",
    "                    f.write(f'{metric}: {value:.4f}\\n')\n",
    "            \n",
    "            print(f\"\\nAll visualizations have been saved to: {save_dir}\")\n",
    "            return performance_summary\n",
    "        # Create visualizations and get performance summary\n",
    "        performance_summary = create_visualization_results(\n",
    "            model=model,\n",
    "            history=history,\n",
    "            test_dataset=test_dataset,\n",
    "            label_encoder=label_encoder,\n",
    "            save_dir='./results'\n",
    "        )\n",
    "\n",
    "        # Print summary to console\n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        print(\"=======================\")\n",
    "        for metric, value in performance_summary.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "        # Save model summary\n",
    "        with open('results/model_summary.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        print(\"\\nModel summary saved as 'results/model_summary.txt'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during training/evaluation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise e"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
